
---

## ğŸ§‘â€ğŸ’» Q1 â€” Vision Transformer on CIFAR-10

### ğŸ”¬ Approach
- Images are split into patches â†’ embedded â†’ passed through transformer blocks.  
- Self-attention captures **global relationships** across patches.  
- Classification is performed using the `[CLS]` token representation.  

### âš™ï¸ Best Model Configuration
| Parameter      | Value |
|----------------|-------|
| Embedding dim  | 256   |
| Depth (layers) | 10    |
| Heads          | 8     |
| Patch size     | 4     |
| Optimizer      | AdamW (lr=3e-4, wd=0.05) |
| Scheduler      | Cosine LR with warmup |
| Epochs         | 80    |

### ğŸ“Š Results
- âœ… **~85% test accuracy** on CIFAR-10.  
- ğŸ“ˆ With larger models (ViT-L/H) and longer training, accuracy can exceed **90%**.  

---

## ğŸ¨ Q2 â€” Text-driven Image Segmentation (CLIPSeg + SAM)

### ğŸ”¬ Pipeline
1. **CLIPSeg (HuggingFace):** Generates a coarse segmentation mask from a text prompt.  
2. **Seed Extraction:** High-confidence regions are converted to seed points.  
3. **SAM (Segment Anything, Meta):** Refines segmentation to produce high-quality masks.  
4. **Visualization:** Final mask is overlaid on the image and saved.  

### ğŸ–¼ï¸ Example
- Input: `dog.jpg`  
- Prompt: `"dog"`  
- Output: `final_mask.png`  

âœ¨ The result is a **refined segmentation mask** corresponding to the text prompt.

---

## âš ï¸ Limitations & Future Work
- **Q1 (ViT):**  
  - Limited compute â†’ trained medium-scale ViT.  
  - ğŸš€ Future: MixUp, CutMix, longer training, and pretrained ViT models for higher accuracy.  

- **Q2 (Segmentation):**  
  - CLIPSeg sometimes fails on vague prompts.  
  - ğŸš€ Future: integrate **GroundingDINO** (text-to-box) + SAM for robust results.  
  - SAM checkpoints are large (~2GB for ViT-H).  

---

## ğŸ’» How to Run in Colab

### 1ï¸âƒ£ Open in Colab
- Go to [Google Colab](https://colab.research.google.com/).  
- Upload `q1.ipynb` or `q2.ipynb`.  
- Enable GPU:  
  `Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU â†’ Save`.

### 2ï¸âƒ£ Run Q1 (Vision Transformer)
- Run cells top-to-bottom.  
- CIFAR-10 dataset auto-downloads.  
- Best checkpoint saved as: **`best_vit_cifar10.pth`**.

### 3ï¸âƒ£ Run Q2 (Segmentation)
- Run cells top-to-bottom.  
- Upload an input image (e.g., `dog.jpg`).  
- Enter a text prompt (e.g., `"dog"`).  
- Output mask saved as **`final_mask.png`**.

---

## ğŸ“Š Results Summary

| Task | Model / Method | Dataset | Result |
|------|----------------|---------|--------|
| Q1   | Vision Transformer (from scratch) | CIFAR-10 | ~85% accuracy |
| Q2   | CLIPSeg + SAM | Custom images | High-quality text-driven segmentation |

---

## ğŸ›  Requirements
All dependencies are installed automatically in the notebooks.  

**Key Libraries:**  
- PyTorch, Torchvision, Timm  
- Transformers (HuggingFace)  
- Segment Anything (Meta AI)  
- OpenCV, Matplotlib, Pillow, Einops  

---

## ğŸ“Œ Credits
- [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)  
- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)  
- [CLIPSeg (HuggingFace)](https://huggingface.co/CIDAS/clipseg-rd64-refined)  
- [Segment Anything (Meta AI)](https://github.com/facebookresearch/segment-anything)  

---

## âœ¨ Final Note
This assignment demonstrates two complementary AI skills:  
- Building **models from scratch** (Q1, Vision Transformer).  
- Combining **pretrained state-of-the-art models** into a creative pipeline (Q2, CLIPSeg + SAM).  

ğŸ“ˆ Together, they highlight the power of both **theory + practice** in modern AI.  
